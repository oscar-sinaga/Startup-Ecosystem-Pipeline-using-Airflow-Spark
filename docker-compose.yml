services:
  airflow:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: airflow_standalone
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_DB_URI}
      - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
    depends_on:
      - airflow_metadata
    ports:
      - "8082:8080"
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - airflow-networks

  airflow_metadata:
    image: postgres:latest
    container_name: airflow_metadata
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB_NAME}
    ports:
      - ${AIRFLOW_PORT}:5432
    volumes:
      - airflow_metadata:/var/lib/postgresql/data
    networks:
      - airflow-networks

  startup_investments_db:
    image: postgres:latest
    hostname: startup_investments_db
    container_name: startup_investments_db
    restart: on-failure
    environment:
      - POSTGRES_DB=${STARTUP_INVESTMENTS_DB_NAME}
      - POSTGRES_USER=${STARTUP_INVESTMENTS_DB_USER}
      - POSTGRES_PASSWORD=${STARTUP_INVESTMENTS_DB_PASSWORD}
    volumes:
      - ./data/source/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - ${STARTUP_INVESTMENTS_PORT}:5432 
    networks:
      - airflow-networks

  minio:
    image: minio/minio:RELEASE.2024-06-13T22-53-53Z
    container_name: minio
    hostname: minio
    restart: always
    volumes:
      - minio-data:/data
    ports:
      - 9000:9000
      - 9001:9001
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - airflow-networks

  staging_db:
    image: postgres:latest
    container_name: staging_db
    environment:
      POSTGRES_USER: ${STAGING_DB_USER}
      POSTGRES_PASSWORD: ${STAGING_DB_PASSWORD}
      POSTGRES_DB: ${STAGING_DB_NAME}
    ports:
      - ${STAGING_PORT}:5432
    networks:
      - airflow-networks
    volumes:
      - staging_db:/var/lib/postgresql/data


  log_db:
    image: postgres:latest
    container_name: log_db
    environment:
      POSTGRES_USER: ${LOG_DB_USER}
      POSTGRES_PASSWORD: ${LOG_DB_PASSWORD}
      POSTGRES_DB: ${LOG_DB_NAME}
    ports:
      - ${LOG_PORT}:5432
    networks:
      - airflow-networks
    volumes:
      - log_db:/var/lib/postgresql/data

  warehouse_db:
    image: postgres:latest
    container_name: warehouse_db
    environment:
      POSTGRES_USER: ${WAREHOUSE_DB_USER}
      POSTGRES_PASSWORD: ${WAREHOUSE_DB_PASSWORD}
      POSTGRES_DB: ${WAREHOUSE_DB_NAME}
    ports:
      - 5437:5432
    networks:
      - airflow-networks
    volumes:
      - warehouse_db:/var/lib/postgresql/data

  spark-master:
    image: bitnami/spark:3.5.5
    command: bin/spark-class org.apache.spark.deploy.master.Master
    container_name: spark-master
    ports:
      - "8089:8080"
      - "7077:7077"
    networks:
      - airflow-networks

  spark-worker-1:
    image: bitnami/spark:3.5.5
    container_name: spark-worker-1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - airflow-networks

  # spark-worker-2:
  #   image: bitnami/spark:3.5.5
  #   container_name: spark-worker-2
  #   command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   environment:
  #     SPARK_MODE: worker
  #     SPARK_WORKER_CORES: 1
  #     SPARK_WORKER_MEMORY: 1g
  #     SPARK_MASTER_URL: spark://spark-master:7077
  #   networks:
  #     - airflow-networks

  # spark-worker-3:
  #   image: bitnami/spark:3.5.5
  #   container_name: spark-worker-3
  #   command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   environment:
  #     SPARK_MODE: worker
  #     SPARK_WORKER_CORES: 1
  #     SPARK_WORKER_MEMORY: 1g
  #     SPARK_MASTER_URL: spark://spark-master:7077
  #   networks:
  #     - airflow-networks

  metabase-db:
    image: postgres:latest
    container_name: ${MB_DB_HOST}
    restart: always
    hostname: ${MB_DB_HOST}
    ports:
      - "5438:5432"  # External 5435 -> Internal 5432
    environment:
      POSTGRES_DB: ${MB_DB_DBNAME}
      POSTGRES_USER: ${MB_DB_USER}
      POSTGRES_PASSWORD: ${MB_DB_PASS}
    networks:
      - airflow-networks
    volumes:
      - metabase_db_data:/var/lib/postgresql/data
      - ./metabase/data:/docker-entrypoint-initdb.d  # Mount for init scripts
      - ./metabase/data:/var/lib/postgresql/backup 

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    restart: always
    ports:
      - "8083:3000"
    env_file:
      - .env
    networks:
      - airflow-networks
    volumes:
      - metabase_data:/metabase-data
    depends_on:
      - metabase-db
    healthcheck:
      test: curl --fail -I http://localhost:8083/api/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 5

volumes:
  airflow_metadata:
    driver: local
    name: airflow_metadata

  dellstore_data:
    driver: local
    name: dellstore_data

  minio-data:
    driver: local
    name: minio-data

  profile_quality_db:
    driver: local
    name: profile_quality_db

  staging_db:
    driver: local
    name: staging_db

  warehouse_db:
    driver: local
    name: warehouse_db

  log_db:
    driver: local
    name: log_db

  metabase_db_data:
    driver: local
    name: metabase_db_data

  metabase_data:
    driver: local
    name: metabase_data

networks:
  airflow-networks:
    driver: bridge
    name: airflow-networks